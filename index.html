<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>PLM-Net: Perception Latency Mitigation Network</title>
  <style>
    body { font-family: sans-serif; max-width: 1200px; margin: 40px auto; line-height: 1.6; }
    h1 { color: #2c3e50; }
    a { color: #2980b9; }
  </style>
</head>
<body>

  <h1>PLM-Net: Perception Latency Mitigation Network for Vision-Based Lateral Control of Autonomous Vehicles</h1>
<!--   <p><strong>Perception Latency Mitigation Network for Vision-Based Lateral Control of Autonomous Vehicles</strong></p> -->

  <p><strong>Authors:</strong> Aws Khalil, and Jaerock Kwon</p>

  <h2>ğŸ“„ Abstract</h2>
  <p>This study introduces the Perception Latency Mitigation Network (PLM-Net), a novel deep learning approach for addressing perception latency in vision-based Autonomous Vehicle (AV) lateral control systems. Perception latency is the delay between capturing the environment through vision sensors (e.g., cameras) and applying an action (e.g., steering). 
    This issue is understudied in both classical and neural-network-based control methods. Reducing this latency with powerful GPUs and FPGAs is possible but impractical for automotive platforms. 
    PLM-Net consists of two models: the Base Model (BM) and the Timed Action Prediction Model (TAPM). BM represents the original Lane Keeping Assist (LKA) system, while TAPM predicts future actions for different latency values. 
    By integrating these models, PLM-Net mitigates perception latency. The final output is determined through linear interpolation of BM and TAPM outputs based on real-time latency. 
    This design addresses both constant and varying latency, improving driving trajectories and steering control. 
    Experimental results validate the efficacy of PLM-Net across various latency conditions.</p>

  <h2>ğŸ”— Paper</h2>
  <p><a href="https://arxiv.org/abs/2407.16740" target="_blank">[View on arXiv]</a></p>

  <h2>ğŸ’» Code</h2>
  <p><a href="https://github.com/AwsKhalil/oscar/tree/devel-plm-net" target="_blank">[GitHub Repository]</a></p>

  <h2>ğŸ“Š Key Results</h2>
  <p>Figures and video coming soon.</p>
  <h2>ğŸ–¼ï¸ Method Overview</h2>
  <img src="images/method-intro.png" alt="PLM-Net architecture" style="width:100%; max-width:800px;">
  
  <h2>ğŸ“Š Evaluation Results</h2>
  
  <h3>Random Trajectory â€“ Steering</h3>
  <img src="images/PLM-delta-rand-steering1-v2.png" alt="Steering comparison - random 1" style="width:50%; max-width:800px;">
  <img src="images/PLM-delta-rand-steering2-v2.png" alt="Steering comparison - random 2" style="width:50%; max-width:800px;">
  
  <h3>Random Trajectory â€“ Trajectory Plot</h3>
  <img src="images/PLM-delta-rand-traj-v2.png" alt="Trajectory plot - random" style="width:100%; max-width:1200px;">
  
  <h3>Fixed Latency (200ms) â€“ Steering</h3>
  <img src="images/PLM-delta-200-steering2-v2.png" alt="Steering comparison - 200ms" style="width:50%; max-width:800px;">
  
  <h3>Fixed Latency (200ms) â€“ Trajectory Plot</h3>
  <img src="images/PLM-delta-200-traj-v2.png" alt="Trajectory plot - 200ms" style="width:100%; max-width:1200px;">

  <h2>ğŸ“¬ Contact</h2>
  <p>If you have questions, feel free to contact: <strong>awskh@umich.edu</strong></p>

</body>
</html>
